---
title: "You can use data.table or tidyverse (or Pandas)!"
author: Erika Duan
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
# Set up global environment ----------------------------------------------------
knitr::opts_chunk$set(echo=TRUE, results="hide", message=FALSE, warning=FALSE)  
```

```{r}
# Load required R packages -----------------------------------------------------
if (!require("pacman")) install.packages("pacman")
pacman::p_load(here,
               tidyverse,
               data.table,
               compare, # Compare results between two data frames   
               microbenchmark,
               reticulate) # Required for the Python environment interface
```

```{r}
# Check Python configurations --------------------------------------------------
# I first created & activated my Python virtual environment using venv [via terminal]
# I then installed pandas inside my virtual environment using pip [via terminal]      

# Force R to use my Python virtual environment
use_virtualenv("C:/Windows/system32/py_396_data_science")

# Use reticulate::py_discover_config() to check Python environment settings 
```


# Introduction   

I started data analysis in `R` with the `tidyverse` suite of packages and `dplyr` still sparks the greatest joy out of all R and Python data wrangling packages. In R, a useful alternative to `dplyr` is `data.table` when you need to perform groupings on large datasets (datasets over 1 million rows).     

The pros and cons of using `tidyverse`, `data.table` and the popular Python data wrangling package `Pandas` are listed below, keeping in mind that some preferences are subjective.      

| Package  | `dplyr` | `data.table` | `Pandas` |   
|:---------|:--------|:-------------|:---------|   
| Code readability | Code is highly readable and integrates well with `%>%` and `\|>` pipes | `data.table` code is more concise, but suffers from decreased readability especially when using long names | `Pandas` allows chaining of methods but its methods syntax can feel less intuitive than `dplyr` |    
| Speed: `group by` | Code fails to execute when doing analysis on groups with ~300K values | Performance remains consistent as the number of groups increase (>300K groups) | Performs equivalently to `dplyr` according to [db-benchmark](https://h2oai.github.io/db-benchmark/) |    
| Speed: `sort` | - | Faster alternative `sort` method to `dplyr` on large datasets (>1M rows) | - |    
| Memory usage | Is copy on modify, which is default R behaviour | Allows modify by reference for some operations, which decreases memory allocation needs | Copy on modify behaviour is enabled for `Pandas` 2.0+ |      

Let's test this out for ourselves.    


# Creating a test dataset   

Imagine you have a dataset describing how students are engaging with online courses:     

+ Each student has a unique ID.  
+ There are 5 different online platforms labelled `A`, `B`, `C`, `D` and `E`.  
+ Students can take multiple courses at any time.  
+ Students have the option of taking different courses within the same platform or switching to a different course on a different platform.   
+ Platform start dates are recorded when the student first signs up to a platform.    
+ Platform end dates are recorded when a student completes their last course or switches to a new platform.   

We can source the R script [`./dc-data_table_vs_dplyr-dataset_generation_script.R`](./dc-data_table_vs_dplyr-dataset_generation_script.R) to generate a mock dataset of 1M course enrollments, which is named `courses_df`.    

```{r}
# Source R script to generate mock dataset -------------------------------------
# Explicitly states that outputs are generated in the global R environment   
source("dc-data_table_vs_dplyr-dataset_generation_script.R",
       local = knitr::knit_global())
```

```{r, results='markup'}
# Preview mock dataset ---------------------------------------------------------
head(courses_df, 5)
```

**Note:** The script `dc-data_table_vs_dplyr-dataset_generation_script.R` may take several minutes to load as it generates 1M synthetic records.   


# Data frame conversions    

The [R script above](./dc-data_table_vs_dplyr-dataset_generation_script.R) generates a `data.table` object named `courses_df`. In R, the default data object is a `data.frame`. A `data.frame` is just a list of vectors (of the same length) with each column represented by a separate vector.      

The `tibble` data object from `dplyr` is a modified version of the `data.frame` that prohibits some column name and type conversions. You can read more about `tibble` design choices in the [Advanced R chapter on tibbles](https://adv-r.hadley.nz/vectors-chap.html?q=tibble#tibble).     

The `data.table` package contains the function `setDT()`, which converts a `data.frame` or tibble into a `data.table` object by reference (without creating a second copy of the original object). 

**Note:** `courses_df` is loaded as a `data.table` object.     

```{r}
# Check data object type -------------------------------------------------------
# Check the class of a data.table object  
typeof(courses_df)
#> [1] "list"  

class(courses_df)
#> [1] "data.table" "data.frame"   

# Check the class of a default R data object created using as.data.frame()
data.frame(id = 1:5, letter = letters[1:5]) |>
  class()
#> [1] "data.frame"  

# Check the class of a tibble data object 
data.frame(id = 1:5, letter = letters[1:5]) |>
  as_tibble() |>  
  class()
#> [1] "tbl_df"     "tbl"        "data.frame"  
```

To showcase equivalent Python `Pandas` code, we will also import the `Pandas` package into our Python environment and create a copy of `course_df` in our Python environment.  

```{python, results='markup'} 
# Import Python packages for Python data transformations -----------------------
import pandas as pd  
import datetime as dt
import numpy as np

pd.options.display.max_rows = 5

# Load courses_df into the Python environment as a Pandas data frame -----------
# Creates a copy of courses_df from the R environment in the Python environment 
courses_pf = r.courses_df

# Convert platform_start_date and platform_end_date to date type ---------------
# Apply a vectorised datetime type conversion for each column containing "date" 

# Extract a list of column names containing "date"  
date_cols = [col for col in courses_pf.columns if "date" in col]

# Create a for loop which converts a column into the YYYY-mm-dd date format
for col in date_cols:
  courses_pf[col] = pd.to_datetime(courses_pf[col], format = "%Y-%m-%d")
  
# Check that the columns are the expected type
courses_pf.dtypes
```


# Code syntax differences   

Before we cover specific examples, let us examine the syntax of `dplyr` and `data.table` functions. The `dplyr` syntax is more beginner friendly as it uses data transformation verbs, which can be chained together to create a new output.   

```{r, echo=FALSE, results="hold", out.width="80%"}
knitr::include_graphics("../../figures/dc-data_table_vs_dplyr-code_syntax.svg")
```

In contrast, the `data.table` syntax is structured in the form `DT[i, j, by]` where:   

+ Data selection (filtering or sorting rows) is performed in the `i` placeholder.     
+ Data column selection, transformation and deletion are performed in the `j` placeholder.   
+ Grouping data is performed in the `by` placeholder, with aggregation functions specified in the `j` placeholder.    


# Filter data    

## Filter by a single condition  

The code to filter data using `dplyr` or `data.table` is very similar and `data.table` objects can also be directly transformed using `dplyr` functions.   

```{r}
# Filter by platform B and C using dplyr ---------------------------------------
courses_df |>
  filter(platform %in% c("B", "C"))

# Filter by platform B and C using data.table ----------------------------------
# The data.table helper operator %chin% is equivalent to but faster than %in%
courses_df[platform %chin% c("B", "C")]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[platform %chin% c("B", "C")] |> head(3)
```

## Filter by multiple conditions   

When filtering by multiple conditions, we use the symbol `|` to denote the expression `OR` and the symbol `&` to denote the expression `AND` (or to be more precise, to fine an union or an intersection between two conditions). In `dplyr`, the comma is a shorthand for `AND`.   

```{r}
# Filter by platform B & platform start date >= 2018-09-01 using dplyr ---------
courses_df |>
  filter(platform == "B",
         platform_start_date >= "2018-09-01")

# Filter by platform B & platform start date >= 2018-09-01 using data.table ----
courses_df[platform == "B" & platform_start_date >= "2018-09-01"]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[platform == "B" & platform_start_date >= "2018-09-01"] |> head(3)
```

## Filter using regular expressions   

We can also filter values using regular expressions via the `stringr` package or the `data.table` helper function `%like%`.    

```{r}
# Filter by course starting with the letter R using dplyr ----------------------
courses_df |>
  filter(str_detect(course, "^R"))

# Filter by course starting with the letter R using data.table -----------------
courses_df[course %like% "R"]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[course %like% "R"] |> head(3)  
```

## Filter across multiple columns  

With the release of [dplyr 1.0.4](https://www.tidyverse.org/blog/2021/02/dplyr-1-0-4-if-any/), we can also filter by a condition across multiple columns using `if_all()` and `if_any()`. An equivalent shortcut does not currently exist for `data.table` or `Pandas`, so columns for filtering still need to be listed manually.    

```{r, results="markup"}
# Filter by platform start AND end date between dates using dplyr --------------
courses_df |>
  filter(if_all(ends_with("_date"), ~between(., "2018-01-01", "2018-03-31"))) |>
  head(3) # View output

# Filter by platform start OR end date between dates using dplyr ---------------
courses_df |>
  filter(if_any(ends_with("_date"), ~between(., "2018-01-01", "2018-03-31"))) |>
  head(3) # View output
```

## Equivalent Pandas code     

```{python, results="markup"} 
# Filter by platform A and B using Pandas in Python ----------------------------
courses_pf[courses_pf.platform.isin(["B", "C"])]  
```

```{python, results="markup"}
# Filter by platform B & platform start date >= 2018-09-01 using Pandas --------
is_platform_B = (courses_pf["platform"] == "B")  
starts_on_or_after_2018_09_01 = (courses_pf["platform_start_date"] >= "2018-09-01")  

courses_pf[is_platform_B & starts_on_or_after_2018_09_01]  
```

In `Pandas`, we can filter values using regular expressions using the [string methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#testing-for-strings-that-match-or-contain-a-pattern) `str.contains`, `str.match` or `str.fullmatch`. In this scenario, the most efficient option is `str.match` to match the first characters of the string.      

```{python, results="markup"}
# Filter by course starting with the letter R using Pandas ---------------------
# Alternatively set regex = True when using df.column.str.contains("pattern") 
courses_pf[courses_pf.course.str.match("R_", na = False)]
```

## Code benchmarking   

The execution time for `dplyr` and `data.table` is roughly equivalent for filtering data.       

```{r, echo=FALSE, results="markup", out.width="60%"} 
# Compare filter examples using dplyr vs data.table ----------------------------
# Perform and plot R code benchmarks 
microbenchmark(
  dplyr_platform_B_start_2018_09_01 = courses_df |>
    filter(platform == "B",
           platform_start_date >= "2018-09-01"),
  dt_platform_B_start_2018_09_01 = courses_df[platform == "B" & 
                                                platform_start_date >= "2018-09-01"],
  dplyr_course_starts_with_R = courses_df |>
    filter(str_detect(course, "^R")),
  dt_course_starts_with_R = courses_df[course %like% "R"],
  times = 15L
) |>
  autoplot()
```
 
 
# Sort data   

Sorting large data frames is computationally expensive, so we should always minimise the number of times we need to sort a column. In `dplyr`, `data.table` and `Pandas`, records are consistently ordered in ascending order by default (likely a design choice from SQL).      

In `dplyr`, we sort columns using `arrange()`. To sort columns by descending order, we append `desc()` inside `arrange().     

The `data.table` package uses an alternative faster sort method accessible via the `order()` function. To sort columns by descending order, we append `-` in front of the column name. We can also sort **by reference** using `setorder()`, which directly modifies the original data frame.    

```{r}
# Sort by ascending student id and descending platform name using dplyr --------
courses_df |>
  arrange(student,
          desc(platform))

# Sort by ascending student id and descending platform name using data.table ---
courses_df[order(student,
                 -platform)]

# setorder() sorts by reference  
# setorder(courses_df, 
#          student,
#          -platform)
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[order(student,
                 -platform)] |> head(4)
```

## Equivalent Pandas code     

In `Pandas`, we use the `.sort_values()` method and can assign missing values to appear in the first or last position via the argument `na_position = "first"`.    

```{python, results='markup'}
# Sort by ascending student id and descending platform name using Pandas -------
courses_pf.sort_values(by = ["student", "platform"],
                       ascending = [True, False], 
                       na_position = "first")
```

## Code benchmarking     

The `data.table` package uses a much faster sort method compared to `dplyr`.         

```{r, echo=FALSE, results="markup", out.width="60%"} 
# Compare sort examples using dplyr vs data.table ------------------------------
# Sort by ascending student id and descending platform name   
# Perform and plot R code benchmarks 
microbenchmark(
  dplyr_sort_student_platform = courses_df |>
    arrange(student, desc(platform)),
  dt_sort_student_platform = courses_df[order(student, -platform)],
  times = 5L
) |>
  autoplot()
```


# Select columns   

Using `dplyr`, single or multiple columns can be selected via `select()` and combined with regular expressions or column type queries wrapped inside [`where()`](https://tidyselect.r-lib.org/reference/where.html). As of `data.table` [version 1.13.0](https://github.com/Rdatatable/data.table/blob/master/NEWS.md#datatable-v1130--24-jul-2020), complex column selections can be performed by inputting a function using `.SDcols`.         

```{r}
# Select multiple columns by name using dplyr ----------------------------------
courses_df |>
  select(student,
         platform)

# Select multiple columns by name using data.table -----------------------------
# .() is a shorthand for list()  
courses_df[,
           .(student,
             platform)]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[,
           .(student,
             platform)] |> head(3)
```

```{r}
# Select columns ending with date using dplyr ----------------------------------
courses_df |>
  select(ends_with("date"))

# Select columns ending with date using data.table -----------------------------
courses_df[, 
           grep("date$", colnames(courses_df), value = TRUE),
           with = FALSE]

# Using .SDcols syntax for functions to output columns via .SD
courses_df[, 
           .SD,
           .SDcols = grep("date$", colnames(courses_df), value = TRUE)]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[, 
           grep("date$", colnames(courses_df), value = TRUE),
           with = FALSE] |> head(3)
```

```{r}
# Select columns of date type using dplyr --------------------------------------
# Apply class(col) across all columns in courses_df to identify column type 
vapply(courses_df, class, character(1L))
#>   student       platform      course        platform_start_date    platform_end_date 
#>   "character"   "character"   "character"   "Date"                 "Date"

# Selects columns where the column type inherits the Date class  
courses_df |>
  select(where(~inherits(., "Date")))

# Select columns of date type using data.table ---------------------------------
# ~inherits(., "Date") is a dplyr shorthand for function(x) inherits(x, 'Date')
courses_df[, 
           .SD,
           .SDcols = function(x) inherits(x, 'Date')]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[, 
           .SD,
           .SDcols = function(x) inherits(x, 'Date')] |> head(3)
```

For `dplyr`, extracting a single column using `select()` will always return another data frame, unless we explicitly use `pull()` to output a vector. For `data.table`, wrapping columns inside a list via `.()` returns another data frame.  

```{r}
# Output data frame with select() using dplyr ----------------------------------
courses_df |>
  select(student) |>
  class()
#> [1] "data.table" "data.frame"  

# Output vector with select() & pull using dplyr -------------------------------
courses_df |>
  pull(student) |>
  class()
#> [1] "character"

# Output data frame following column selection using data.table ----------------
courses_df[, .(student)] |>
  class()
#> [1] "data.table" "data.frame"  

# Output vector following column selection using data.table --------------------
courses_df[, student] |>
  class() 
#> [1] "character"
```

## Equivalent Pandas code      

In `Pandas`, the `.filter()` method can be used to select columns using regular expressions and the `.select_dtypes()` method is used to select columns by column type. `Pandas` data frames can be outputted as a data frame, series or NumPy array.        

```{python, results='markup'}
# Select multiple columns by name using Pandas ---------------------------------
courses_pf[["student", "platform"]]
```

```{python, results='markup'}
# Select columns ending with date using Pandas ---------------------------------
courses_pf.filter(regex = ("date$"))  
```

```{python, results='markup'}
# Select columns of date type using Pandas -------------------------------------
# Access the data frame .dtypes attribute to first identify all column types  
# courses_pf.dtypes
courses_pf.select_dtypes(include=["datetime64"])
```

```{python}
# Output data frame following column selection using Pandas --------------------
type(courses_pf[["student"]])
#> <class 'pandas.core.frame.DataFrame'>  

# Output series or vector array following column selection using Pandas --------
type(courses_pf["student"])
#> <class 'pandas.core.series.Series'>  

type(courses_pf["student"].values)
#> <class 'numpy.ndarray'>
```

## Code benchmarking   

The execution time for `dplyr` and `data.table` is roughly equivalent for selecting columns, with `dplyr` exhibiting slightly faster code execution times.     

```{r, echo=FALSE, results="markup", out.width="60%"} 
# Compare column selection examples using dplyr vs data.table ------------------
# Perform and plot R code benchmarks 
microbenchmark(
  dplyr_select_two_cols = courses_df |>
    select(student,
           platform),
  dt_select_two_cols = courses_df[,
                                  .(student, platform)],
  dplyr_select_date_type = courses_df |>
    select(where(~inherits(., "Date"))),
  dt_select_date_type = courses_df[, 
                                   .SD,
                                   .SDcols = function(x) inherits(x, 'Date')],
  times = 15L
) |>
  autoplot()
```


# Transform columns    

## Transform via copy on modify versus modify in place       

Using `dplyr`, we can create new columns or overwrite existing columns in our original data frame using `mutate()`, or create or overwrite new columns and drop all other existing columns using `transmute()`.   

```{r, results='markup'}
# Create column for platform dwell length using dplyr --------------------------
courses_df |> 
  mutate(platform_dwell_length = platform_end_date - platform_start_date) |>
  head(3)  

courses_df |> 
  transmute(platform_dwell_length = platform_end_date - platform_start_date) |>
  head(3)  
```

In R, a second copy of an object is created by default whenever the original object is modified and this behaviour is called [copy on modify](https://adv-r.hadley.nz/names-values.html#copy-on-modify). This implies means that a shallow data.frame copy is created whenever a column is modified using `mutate()` with `dplyr`. We can trace this behaviour using `lobstr::ref()` according to [chapter 2.3.4](https://adv-r.hadley.nz/names-values.html#df-modify) of the [Advanced R textbook](https://adv-r.hadley.nz/index.html).    

```{r}
# Check copy on modify behaviour following mutate() ----------------------------
lobstr::ref(courses_df)
#> o [1:0x1a043067b00] <dt[,5]> 
#> +-student = [2:0x7ff450310010] <chr> 
#> +-platform = [3:0x7ff44fb60010] <chr> 
#> +-course = [4:0x7ff44f3b0010] <chr> 
#> +-platform_start_date = [5:0x7ff44ec00010] <date> 
#> \-platform_end_date = [6:0x7ff44e450010] <date> 

# Modify the platform column and check data frame and column references --------
courses_df <- courses_df |>
  mutate(platform = tolower(platform))  

lobstr::ref(courses_df)
#> o [1:0x1a043924e28] <dt[,5]> 
#> +-student = [2:0x7ff450310010] <chr> 
#> +-platform = [3:0x7ff45c2d0010] <chr> 
#> +-course = [4:0x7ff44f3b0010] <chr> 
#> +-platform_start_date = [5:0x7ff44ec00010] <date> 
#> \-platform_end_date = [6:0x7ff44e450010] <date>    

# A new copy of the courses_df data frame as well as the platform column is 
# created and referenced.  
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
# courses_df has already been modified so we want to view the result
head(courses_df, 3)
```

Using `data.table`, we can directly modify or even remove columns in place using the `:=` operator inside the `j` placeholder of `DT[i, j, by]`. This is a special property of `data.table` and not `dplyr` or `Pandas`.    

```{r}
# Check modify in place behaviour following := assignment ----------------------
lobstr::ref(courses_df)  
#> o [1:0x1a043924e28] <dt[,5]> 
#> +-student = [2:0x7ff450310010] <chr> 
#> +-platform = [3:0x7ff45c2d0010] <chr> 
#> +-course = [4:0x7ff44f3b0010] <chr> 
#> +-platform_start_date = [5:0x7ff44ec00010] <date> 
#> \-platform_end_date = [6:0x7ff44e450010] <date>  

# Modify the platform column and check data frame and column references --------
courses_df[,
           platform := toupper(platform)]

lobstr::ref(courses_df)
#> o [1:0x1a043924e28] <dt[,5]> 
#> +-student = [2:0x7ff450310010] <chr> 
#> +-platform = [3:0x7ff459750010] <chr> 
#> +-course = [4:0x7ff44f3b0010] <chr> 
#> +-platform_start_date = [5:0x7ff44ec00010] <date> 
#> \-platform_end_date = [6:0x7ff44e450010] <date> 

# By modifying columns in place, we do not create a new copy of the courses_df 
# data frame every time we modify a column. 

# Modify multiple columns using data.table -------------------------------------
# To modify multiple columns in place, we move `:=` to the front expression    
courses_df[,
           `:=` (platform_dwell_length = platform_end_date - platform_start_date,
                 platform_start_year = str_extract(platform_start_date, "^.{4}(?!//-)"))]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
# courses_df has already been modified in place so we want to view the result
head(courses_df, 3)
```

## Transform using multiple conditions    

We can create a column based on multiple conditions using `case_when()` or `f_case()` for `dplyr` and `data.table` respectively.   

```{r}
# Create column based on multiple conditions using dplyr -----------------------
courses_df |>
  mutate(studied_programming = case_when(
    str_detect(course, "^R_") ~ "Studied R",
    str_detect(course, "^Python_") ~ "Studied Python",
    TRUE ~ "No"
  ))

# Create column based on multiple conditions using data.table ------------------
courses_df[,
           studied_programming := fcase(
             str_detect(course, "^R_"), "Studied R",
             str_detect(course, "^Python_"), "Studied Python",
             default = "No")]  
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
# courses_df has already been modified in place so we want to view the result
head(courses_df, 3)
```

```{r}
# Remove columns in place using data.table -------------------------------------
# Columns can be removed in place by using := NULL column assignment  
courses_df[,
           c("platform_start_year",
             "studied_programming") := NULL]
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
# courses_df has already been modified in place so we want to view the result
head(courses_df, 3)
```

Since `dplyr` [version 1.0.0](https://www.tidyverse.org/blog/2020/06/dplyr-1-0-0/), we can use `mutate()` in combination with `across()` to apply the same transformation functions across multiple columns. In `data.table`, the equivalent solution is to specify columns of interest via `.SDcols` and then loop through each column using `lapply(function)`.     

```{r}
# Apply the same function across multiple columns using dplyr ------------------
courses_df |>
  mutate(across(c(student, platform, course), tolower))
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df |>
  mutate(across(c(student, platform, course), tolower)) |> head(3)
```

```{r}
# where() can be used inside across() for conditional column selections  
courses_df |>
  mutate(across(where(is.character), toupper))
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df |>
  mutate(across(where(is.character), toupper)) |> head(3)
```

```{r}
# Apply the same function across multiple columns using data.table -------------
# data.table modifies all selected columns in place via :=
cols <- c("student", "platform", "course")
courses_df[,
           (cols) := lapply(.SD, tolower),
           .SDcols = cols]  
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
# courses_df has already been modified in place so we want to view the result
head(courses_df, 3)
```


## Equivalent Pandas code      

In `Pandas`, we can transform columns directly by assignment or by using the `.apply()` method. The benefit of using `.apply()` is that it also handles bespoke functions. A [Stack Overflow post](https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas) discussing the different uses of `.apply()` and `.applymap()`.      

```{python}
# Create column for platform dwell length using Pandas -------------------------
courses_pf["platform_dwell_length"] = courses_pf["platform_end_date"] - courses_pf["platform_start_date"]
```

```{python, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_pf
```

```{python}
# Create column based on multiple conditions using Pandas ----------------------
# Use np.select() rather than .apply() to apply a vectorised transformation
courses_pf["studied_programming"] = np.select(
  [courses_pf["course"].str.match("R_", na = False), 
   courses_pf["course"].str.match("Python_", na = False)],
  ["Studied R",
   "Studied Python"],
  default = "No"
)
```

```{python, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_pf
```

```{python}
# Apply the same function across multiple columns using Pandas -----------------
# Applying a vector operation looped over a list of columns is faster than 
# transforming columns element-wise using .apply().  

# Avoid .apply() unless you need to apply a function element-wise across a column
# For example, the comments code below runs significantly slower    
# courses_pf[object_cols] = courses_pf[object_cols].apply(lambda col: col.str.upper()) 

object_cols = [col for col, dt in courses_pf.dtypes.items() if dt == "object"]

for col in object_cols: 
  courses_pf[col] = courses_pf[col].str.lower()
```

```{python, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_pf
```

```{python}
# Remove columns using Pandas --------------------------------------------------
courses_pf = courses_pf.drop(columns=["studied_programming"])
```

## Code benchmarking   

The execution time for `dplyr` and `data.table` is roughly equivalent for transforming columns, with `data.table` exhibiting slightly faster code execution times.   

```{r, echo=FALSE, results="markup", out.width="60%"} 
# Compare transformation examples using dplyr vs data.table --------------------
# Perform and plot R code benchmarks 
microbenchmark(
  dplyr_create_platform_dlength = courses_df |> 
    mutate(platform_dwell_length = platform_end_date - platform_start_date),
  dt_create_platform_dlength = courses_df[,
                                          platform_dwell_length := platform_end_date - platform_start_date],
  dplyr_case_when = courses_df |>
    mutate(studied_programming = case_when(
      str_detect(course, "^R_") ~ "Studied R",
      str_detect(course, "^Python_") ~ "Studied Python",
      TRUE ~ "No")),
  dt_fcase = courses_df[,
                        studied_programming := fcase(
                          str_detect(course, "^R_"), "Studied R",
                          str_detect(course, "^Python_"), "Studied Python",
                          default = "No")],
  times = 5L
) |>
  autoplot()
```

# Group by and summarise columns     

Summarising data across >100K subgroups is where `data.table` greatly outperforms `dplyr` in terms of code execution speed. In `dplyr`, a grouping convention is specified using the `group_by()` function and summary outputs then created using the `summarise()` function.   

```{r}
# Summarise by course and platform using dplyr ---------------------------------
courses_df |>
  group_by(platform, course) |>
  summarise(mean_length = mean(platform_dwell_length),
            total_courses = n()) 

# dplyr cannot group by student as it has >200K subgroups whereas data.table can 

# Summarise by course and platform using data.table ----------------------------
courses_df[,
           .(mean_length = mean(platform_dwell_length),
             total_courses = .N),
           by = .(platform, course)] 
```

```{r, echo=FALSE, results='markup'}
# Silently print code results --------------------------------------------------
courses_df[,
           .(mean_length = mean(platform_dwell_length),
             total_courses = .N),
           by = .(platform, course)] |> head(3)
```

Since `dplyr` [version 1.0.0](https://www.tidyverse.org/blog/2020/06/dplyr-1-0-0/), we can use `summarise()` in combination with `across()` to apply the same aggregation across multiple columns. A comprehensive [blog post] on `across()` examples can be found [here](https://willhipson.netlify.app/post/dplyr_across/dplyr_across/).      

## Equivalent Pandas code   

In `Pandas`, we can summarise data across subgroups using the method `.groupby()` followed by the method `.agg()`.   

```{python, results='markup'}
# Summarise by course and platform using Pandas --------------------------------
courses_pf.groupby(["platform", "course"]).agg(mean_length = ("platform_dwell_length", "mean"),
                                               total_course = ("platform_dwell_length", "count"))
```

## Code benchmarking     

The execution time for `dplyr` and `data.table` is roughly equivalent for transforming columns, with `data.table` exhibiting slightly faster code execution times.   

```{r, echo=FALSE, results="markup", out.width="60%"} 
# Compare group aggregation examples using dplyr vs data.table -----------------
# Perform and plot R code benchmarks 
microbenchmark(
  dplyr_summarise_by_platform_and_course = courses_df |>
    group_by(platform, course) |>
    summarise(mean_length = mean(platform_dwell_length),
              total_courses = n()),
  dt_summarise_by_platform_and_course = courses_df[,
                                                   .(mean_length = mean(platform_dwell_length),
                                                     total_courses = .N),
                                                   by = .(platform, course)],
  times = 5L
) |>
  autoplot()
```

# Chain code    

In data analysis, we frequently need to perform a sequence of operations to generate a new named data output. In R, we can chain `dplyr` and `data.table` operations using `%>%` or `|>` pipes or `data.table` `[]` chains. I personally prefer using `|>` for chaining R operations when I don't care about minimising package dependencies.     

```{r}
# Chain dplyr operations using |> and |> pipes --------------------------------
# Output a data frame containing the earliest platform start date for all 
# courses per platform  
courses_df |>
  select(-c(student, platform_dwell_length, platform_end_date)) |>
  arrange(platform, course, platform_start_date) |>
  group_by(platform, course) |>
  filter(row_number() == 1L) 

# You can also use the base R pipe |> for R version >= 4.1.0
courses_df |>
  select(-c(student, platform_dwell_length, platform_end_date)) |>
  arrange(platform, course, platform_start_date) |>
  group_by(platform, course) |>
  filter(row_number() == 1L) 

# Chain data.table operations using %>% ----------------------------------------
# The native R pipe |> does not work on the code below
courses_df[,
           -c("student",
              "platform_dwell_length",
              "platform_end_date",
              "studied_programming")] %>% 
  .[order(platform, course, platform_start_date)] %>%
  .[,
    .SD[1L],
    by = .(platform, course)]

# A base R pipe solution currently does not exist for data.table  

# Chain data.table operations using [] -----------------------------------------
courses_df[,
           -c("student",
              "platform_dwell_length",
              "platform_end_date",
              "studied_programming")
][
  order(platform,
        course,
        platform_start_date)
][
  ,
  .SD[1L],
  by = .(platform, course)]
```

## Equivalent Pandas code     

In Python, we can also chain Pandas methods inside `()`. Note that this practice currently only works on Pandas methods and is therefore a lot less flexible than using pipes in R.    

```{python}
# Chain Pandas operations using () ---------------------------------------------
# Multiple Pandas methods can be chained inside a single bracket 
(
  courses_pf
  .sort_values(by = ["platform", "course", "platform_start_date"])
  .groupby(["platform", "course"])
  .first()
  .drop(columns=["student", "platform_dwell_length", "platform_end_date"])
)
```


# Other resources     

+ The official `data.table` [vignette](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).    
+ A comprehensive [stack overflow discussion](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly/27840349#27840349) about the advantages of using `data.table` versus `dplyr`.   
+ A [blog post](https://atrebas.github.io/post/2019-03-03-datatable-dplyr/) with side by side comparison of data.table versus dplyr operations by Atrebas.    
+ An [explanation](https://tysonbarrett.com//jekyll/update/2019/07/12/datatable/) of how `data.table` modifies by reference by Tyson Barrett.   
+ A [detailed explanation](https://gist.github.com/arunsrinivasan/dacb9d1cac301de8d9ff) of binary search based subsets in `data.table` by Arun Srinivasan.      
+ The Advanced R [chapter](https://adv-r.hadley.nz/perf-measure.html) on measuring code performance by Hadley Wickham.   